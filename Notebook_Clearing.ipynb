{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Data cleaning"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["46\n","Index(['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10',\n","       'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19',\n","       'PC20', 'PC21', 'PC22', 'PC23', 'PC24', 'PC25', 'PC26', 'PC27', 'PC28',\n","       'PC29', 'PC30', 'PC31', 'PC32', 'PC33', 'PC34', 'PC35', 'PC36', 'PC37',\n","       'PC38', 'PC39', 'PC40', 'PC41', 'PC42', 'PC43', 'PC44', 'PC45', 'PC46',\n","       'PC47', 'PC48', 'PC49', 'PC50', 'PC51', 'PC52', 'PC53', 'PC54'],\n","      dtype='object')\n"]}],"source":["import pandas as pd\n","import numpy as np # Numerical computation package\n","np.random.seed(1) # Set the random seed for reproduceability\n","\n","ped = pd.read_csv(\"köln-schildergasse (west)-20180430-20200131-hour.csv\", sep = \";\")\n","holi = pd.read_csv(\"Feiertage_2018.01.01_2020.01.31.csv\", sep = \";\")\n","##########################################print(ped.head(3))\n","##########################################print(holi.head(3))\n","#test\n","\n","\n","#########################################################################\n","#Machen wir das in eine Neue Box oder nicht? Die Print sachen können wir sonst raus nehmen?\n","#########################################################################\n","\n","\n","#Splitting up the Time of measurement into date and time\n","ped[[\"date\", \"time\"]] = ped[\"time of measurement\"].str.split(\" \", n=1, expand=True)\n","#Drpo Time of measuremnt, because it is split now and drop location\n","ped = ped.drop([\"time of measurement\", \"location\"], axis=1)\n","#Drop incidents, if only nan\n","ped.dropna(how='all', axis=1, inplace=True)\n","\n","# restructure by date\n","ped = ped[ [\"date\", \"time\"] + [ col for col in ped.columns if col != \"date\" and col!= \"time\"] ]\n","#change name\n","holi = holi.rename(columns={\"name\": \"holiday\"})\n","#merge on date\n","ped = pd.merge(ped, holi, how = \"left\")\n","\n","#put hollyday as 1 and no holyday as 0\n","ped[\"holiday\"] = ped[\"holiday\"].fillna(0)\n","ped[\"holiday\"] = ped[\"holiday\"].where(ped[\"holiday\"] == 0, 1)\n","\n","# clear time from the minute variation\n","ped[\"time\"] = ped[\"time\"].str.slice_replace(8, 16)\n","\n","# Get a better insight into the weather data and missing data\n","##########################################print(ped[\"weather condition\"].unique())\n","##########################################print(ped.isnull().sum() / ped.shape[0])\n","\n","#See where the data are missing\n","null_data = ped[ped.isnull().any(axis=1)]\n","##########################################print(null_data)\n","\n","\n","#########################################################################\n","#Machen wir das in eine Neue Box oder nicht? Die Print sachen können wir sonst raus nehmen?\n","#########################################################################\n","\n","\n","#because the data for temperature and weather condition are both missing, this can be dropped, due to the huge sample size\n","ped.dropna(inplace=True)\n","\n","#Import datapackages\n","from datetime import date, timedelta\n","from datetime import datetime\n","\n","#Import Dataframe from the holidays\n","school_holi = pd.read_csv(\"OpenData_Ferientermine.csv\", sep = \";\")\n","days_df_list = []\n","\n","#Create dataframe with all dates singular listed \n","for i in range(school_holi.shape[0]):\n","    #Get the start and enddate from the holidays\n","    start_date = datetime.strptime(school_holi[\"ErsterTagDate\"][i], \"%d.%m.%Y\").date()\n","    end_date = datetime.strptime(school_holi[\"LetzterTagDate\"][i], \"%d.%m.%Y\").date()\n","\n","    #create dataframes out of the range and append to a list\n","    delta = end_date - start_date\n","    days = [start_date + timedelta(days=n) for n in range(delta.days + 1)]\n","    days_df = pd.DataFrame (days, columns = ['date'])\n","    days_df_list.append(days_df)\n","\n","#create single dataframe out of the list and add value 1 for holiday\n","new_holi = pd.concat(days_df_list).reset_index(drop=True)\n","new_holi[\"school holiday\"] = 1\n","new_holi[\"date\"] = new_holi[\"date\"].astype(str)\n","\n","#merg of the two frames\n","ped = pd.merge(ped, new_holi, how = \"left\", on=\"date\")\n","ped[\"school holiday\"] = ped[\"school holiday\"].fillna(0)\n","\n","#change float to int\n","ped[\"school holiday\"] = np.int64(ped[\"school holiday\"].astype(int))\n","\n","#Get Column with months\n","ped[\"months\"] = ped[\"date\"].str.slice(5, 7)\n","\n","#Transform time to hours\n","ped[\"hour\"] = ped[\"time\"].str.slice(0, 2)\n","\n","\n","# Save for the API\n","ped.to_csv(\"data_clean_without_dummies.csv\", sep = \";\")\n","\n","\n","#get dummies for models\n","ped = pd.get_dummies(ped, columns=[\"weekday\", \"weather condition\", \"months\", \"hour\"])\n","\n","\n","#########################################################################\n","#Machen wir das in eine Neue Box oder nicht? Die Print sachen können wir sonst raus nehmen?\n","#########################################################################\n","\n","#Creating the PCA and look if we can safe data storage\n","from sklearn.decomposition import PCA\n","\n","# For PCA, we want to standardize our data, i.e., de-mean it and divide it by the standard deviation\n","standardize = lambda x: (x - x.mean()) / x.std()\n","ped[\"temperature in ºc\"] = standardize(ped[\"temperature in ºc\"])\n","\n","\n","###################################################################################\n","#Dieser Teil ist für die PCA (klappt das alles?), Die PC sind alle hinten dran gehängt\n","###################################################################################\n","#Safe all features to calculate the cpa\n","features = ped.columns[3:] # Features\n","\n","# Column names for our principal components\n","principal_components = [f\"PC{i+1}\" for i in range(len(features))]\n","\n","# Run PCA with 4 principal components on our iris data\n","pca = PCA(n_components=len(features))\n","ped[principal_components] = pca.fit_transform(ped[features])\n","\n","\n","###################################################################################\n","#57 sind die Columns, ab dem pca los geht und k giebt an, wie viele benötigt werden für die Schwelle\n","###################################################################################\n","threshhold = 0.99\n","k = np.argmax(pca.explained_variance_ratio_.cumsum() >= threshhold)\n","print(k)\n","print(ped.columns[57:])\n","###################################################################################\n","\n","#save prepeared data\n","ped.to_csv(\"data_clean_with_dummies.csv\")\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.6 ('dsf')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"3d263d15b78a78e170f08f86091fdf3477bff66b18e3e3298508eecfc9ac5163"}}},"nbformat":4,"nbformat_minor":2}
