{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup** <br> In this part the important packages and the data are going to be imported. The data is going to be preprocessed, so we are able to work with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the necessary packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "from sklearn.neural_network import MLPRegressor # neural network that is going to be used\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the data and splitting it into a train and test set to work with\n",
    "# important!! use index_col=0 to read df otherwise pca starts at col 58\n",
    "ped = pd.read_csv(\"data_clean_with_dummies.csv\", index_col=0)\n",
    "\n",
    "# beginning of the code, so everyone has the same data, for reproduceability\n",
    "np.random.seed(1) \n",
    "\n",
    "# Define a new X with the squared feature k = 0.99, 57+46\n",
    "X = np.array(ped[ped.columns[57:103]])\n",
    "\n",
    "# Output to predict\n",
    "y = ped[\"pedestrians count\"]\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter tuning** <br> The different hyperparameters of the used function for our neural netwwork (MLPRegressor by sklearn) are being optimized. <br> 1. Establish a base scenario with the standard parameters to compare afterwards. <br> 2. Analyze the influence of the different parameters of MLPRegressor if every other parameter is in default. <br> 3. Trying to find the best parameters using RandomizedSearchCV and GridSearchCV using subsamples of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*1. Establish Base Scenario*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a neural network with the default parameters \n",
    "nnet_base = MLPRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data to work so it is less sensitive to feature scaling \n",
    "scaler = StandardScaler()\n",
    "# scale the inputs\n",
    "scaler.fit(Xtrain)\n",
    "Xtrain_scaled = scaler.transform(Xtrain)\n",
    "# apply the same transformation to the test data to have meaningful results\n",
    "scaler.fit(Xtest)\n",
    "Xtest_scaled = scaler.transform(Xtest)\n",
    "# scale the y different because we can't use standardscaler to a 1d array\n",
    "mu, sigma = y.mean(), y.std()\n",
    "ytest_scaled = (ytest - mu) / sigma\n",
    "ytrain_scaled = (ytrain - mu) /sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>mae_train</th>\n",
       "      <th>mae_test</th>\n",
       "      <th>r2_train</th>\n",
       "      <th>r2_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nnet_base</td>\n",
       "      <td>347.447001</td>\n",
       "      <td>456.57609</td>\n",
       "      <td>0.964814</td>\n",
       "      <td>0.942335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model   mae_train   mae_test  r2_train   r2_test\n",
       "0  nnet_base  347.447001  456.57609  0.964814  0.942335"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the network to the scaled train data\n",
    "nnet_base.fit(Xtrain_scaled, ytrain_scaled)\n",
    "\n",
    "# Make predictions\n",
    "ypred_train_scaled = nnet_base.predict(Xtrain_scaled)\n",
    "ypred_scaled = nnet_base.predict(Xtest_scaled)\n",
    "\n",
    "# Reconstruct outputs and scale back predictions\n",
    "ytest = ytest_scaled * sigma + mu\n",
    "ytrain = ytrain_scaled * sigma + mu\n",
    "ypred_train = ypred_train_scaled * sigma + mu\n",
    "ypred = ypred_scaled * sigma + mu\n",
    "# the network predicts negative values because it is not possible to put an activation function on the output layer\n",
    "# MLPRegressor uses the activation function only on the hidden layers\n",
    "# Negative Values make no sense in our case, therefore we fix it and tell the network that all negative values\n",
    "# should equal to zero\n",
    "ypred[ypred < 0] = 0\n",
    "\n",
    "# Compute the MAE\n",
    "mae_train = mean_absolute_error(ytrain, ypred_train)\n",
    "mae_test = mean_absolute_error(ytest, ypred)\n",
    "# Comute R^2\n",
    "r2_train = r2_score(ytrain, ypred_train)\n",
    "r2_test = r2_score(ytest, ypred)\n",
    "\n",
    "# creating a dataframe to safe all results of mae and r2\n",
    "comparison = pd.DataFrame(columns=[\"model\", \"mae_train\", \"mae_test\", \"r2_train\", \"r2_test\"])\n",
    "# adding the calculated reults to the dataframe\n",
    "row = pd.DataFrame(data=[[\"nnet_base\",  mae_train, mae_test, r2_train, r2_test]],\n",
    "columns=[\"model\", \"mae_train\", \"mae_test\", \"r2_train\", \"r2_test\"])\n",
    "comparison = pd.concat([comparison, row])\n",
    "# merging results into dataframe and safe it as csv to add things later\n",
    "comparison = comparison.reset_index().drop(columns=\"index\")\n",
    "comparison.to_csv(\"nnet_comparison.csv\", sep=\";\", index=False)\n",
    "comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2. Analyze the Influence of the Different Parameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduncting the parameter analysis on the most used parameters for optimization\n",
    "# how many hidden layers with how many neurons are we going to use\n",
    "# 100 is the default value, other values chosen randomly to see the effect of more layers\n",
    "hidden_layer_sizes = [(100,), (32, 64, 32), (64, 128, 64), (32, 64, 128, 64, 32)]\n",
    "# activation function that is going to be used on the hidden layers, relu is the default value\n",
    "activations = [\"relu\", \"logistic\", \"tanh\", \"identity\"]\n",
    "# the solver which optimizes the weights, adam is the default value\n",
    "solvers = [\"sgd\", \"adam\"]\n",
    "# regularization: avoids overfitting, 0.0001 is the default value\n",
    "alphas = np.linspace(0.00001, 0.001, 50)\n",
    "# how high is the initial learing rate, 0.001 is the default value\n",
    "learing_rate_inits = np.linspace(0.001, 0.01, 50)\n",
    "# number of epochs, 200 is the default value\n",
    "# often it doesn't converge then so we start at 1000\n",
    "max_iters = range(1000, 2000, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe to save the results\n",
    "results = pd.DataFrame(columns=[\"Parameter\", \"ParameterValue\", \"MaeTrain\", \"MaeTest\", \"R2Train\", \"R2Test\"])\n",
    "\n",
    "# defining a function to do the analysis on the different parameters\n",
    "def test_parameter(param, param_values, initialize_model):\n",
    "    global results\n",
    "    for param_value in param_values:\n",
    "        nnet_param = initialize_model(param_value)\n",
    "        # Fit the network to the train data\n",
    "        nnet_param.fit(Xtrain_scaled, ytrain_scaled)\n",
    "        # Make predictions\n",
    "        ypred_param_train_scaled = nnet_param.predict(Xtrain_scaled)\n",
    "        ypred_param_scaled = nnet_param.predict(Xtest_scaled)\n",
    "\n",
    "        # Reconstruct outputs and scale predictions\n",
    "        ypred_param_train = ypred_param_train_scaled * sigma + mu\n",
    "        ypred_param = ypred_param_scaled * sigma + mu\n",
    "        ypred_param[ypred_param < 0] = 0 # all neg values equal zero to make sense\n",
    "        \n",
    "        # Compute the MAE\n",
    "        mae_param_train = mean_absolute_error(ytrain, ypred_param_train)\n",
    "        mae_param_test = mean_absolute_error(ytest, ypred_param)\n",
    "        # Comute R^2\n",
    "        r2_param_train = r2_score(ytrain, ypred_param_train)\n",
    "        r2_param_test = r2_score(ytest, ypred_param)\n",
    "\n",
    "        # adding the calculated reults to the dataframe\n",
    "        row = pd.DataFrame(data=[[param, param_value, mae_param_train, mae_param_test, r2_param_train, r2_param_test]],\n",
    "        columns=[\"Parameter\", \"ParameterValue\", \"MaeTrain\", \"MaeTest\", \"R2Train\", \"R2Test\"])\n",
    "        results = pd.concat([results, row])\n",
    "\n",
    "# apply the function to the different parameters\n",
    "test_parameter(\"hidden_layer_size\", hidden_layer_sizes, lambda value: MLPRegressor(hidden_layer_sizes=value, random_state=72))\n",
    "test_parameter(\"activation\", activations, lambda value: MLPRegressor(activation=value, random_state=72))\n",
    "test_parameter(\"solver\", solvers, lambda value: MLPRegressor(solver=value, random_state=72))\n",
    "test_parameter(\"alpha\", alphas, lambda value: MLPRegressor(alpha=value, random_state=72))\n",
    "test_parameter(\"learning_rate_init\", learing_rate_inits, lambda value: MLPRegressor(learning_rate_init=value, random_state=72))\n",
    "test_parameter(\"max_iter\", max_iters, lambda value: MLPRegressor(max_iter=value, random_state=72))\n",
    "\n",
    "# merging results into dataframe and safe it as csv to work with it later on\n",
    "results = results.reset_index().drop(columns=\"index\")\n",
    "results.to_csv(\"param_testing_nnet.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parameter</th>\n",
       "      <th>ParameterValue</th>\n",
       "      <th>MaeTrain</th>\n",
       "      <th>MaeTest</th>\n",
       "      <th>R2Train</th>\n",
       "      <th>R2Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hidden_layer_size</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>347.696832</td>\n",
       "      <td>437.135880</td>\n",
       "      <td>0.964892</td>\n",
       "      <td>0.943203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hidden_layer_size</td>\n",
       "      <td>(32, 64, 32)</td>\n",
       "      <td>297.319744</td>\n",
       "      <td>427.635196</td>\n",
       "      <td>0.972019</td>\n",
       "      <td>0.931630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hidden_layer_size</td>\n",
       "      <td>(64, 128, 64)</td>\n",
       "      <td>297.511867</td>\n",
       "      <td>427.921575</td>\n",
       "      <td>0.973874</td>\n",
       "      <td>0.927907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hidden_layer_size</td>\n",
       "      <td>(32, 64, 128, 64, 32)</td>\n",
       "      <td>244.434975</td>\n",
       "      <td>442.420122</td>\n",
       "      <td>0.979625</td>\n",
       "      <td>0.921200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>activation</td>\n",
       "      <td>relu</td>\n",
       "      <td>347.696832</td>\n",
       "      <td>437.135880</td>\n",
       "      <td>0.964892</td>\n",
       "      <td>0.943203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>max_iter</td>\n",
       "      <td>1900</td>\n",
       "      <td>347.696832</td>\n",
       "      <td>437.135880</td>\n",
       "      <td>0.964892</td>\n",
       "      <td>0.943203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>max_iter</td>\n",
       "      <td>1920</td>\n",
       "      <td>347.696832</td>\n",
       "      <td>437.135880</td>\n",
       "      <td>0.964892</td>\n",
       "      <td>0.943203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>max_iter</td>\n",
       "      <td>1940</td>\n",
       "      <td>347.696832</td>\n",
       "      <td>437.135880</td>\n",
       "      <td>0.964892</td>\n",
       "      <td>0.943203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>max_iter</td>\n",
       "      <td>1960</td>\n",
       "      <td>347.696832</td>\n",
       "      <td>437.135880</td>\n",
       "      <td>0.964892</td>\n",
       "      <td>0.943203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>max_iter</td>\n",
       "      <td>1980</td>\n",
       "      <td>347.696832</td>\n",
       "      <td>437.135880</td>\n",
       "      <td>0.964892</td>\n",
       "      <td>0.943203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Parameter         ParameterValue    MaeTrain     MaeTest  \\\n",
       "0    hidden_layer_size                 (100,)  347.696832  437.135880   \n",
       "1    hidden_layer_size           (32, 64, 32)  297.319744  427.635196   \n",
       "2    hidden_layer_size          (64, 128, 64)  297.511867  427.921575   \n",
       "3    hidden_layer_size  (32, 64, 128, 64, 32)  244.434975  442.420122   \n",
       "4           activation                   relu  347.696832  437.135880   \n",
       "..                 ...                    ...         ...         ...   \n",
       "155           max_iter                   1900  347.696832  437.135880   \n",
       "156           max_iter                   1920  347.696832  437.135880   \n",
       "157           max_iter                   1940  347.696832  437.135880   \n",
       "158           max_iter                   1960  347.696832  437.135880   \n",
       "159           max_iter                   1980  347.696832  437.135880   \n",
       "\n",
       "      R2Train    R2Test  \n",
       "0    0.964892  0.943203  \n",
       "1    0.972019  0.931630  \n",
       "2    0.973874  0.927907  \n",
       "3    0.979625  0.921200  \n",
       "4    0.964892  0.943203  \n",
       "..        ...       ...  \n",
       "155  0.964892  0.943203  \n",
       "156  0.964892  0.943203  \n",
       "157  0.964892  0.943203  \n",
       "158  0.964892  0.943203  \n",
       "159  0.964892  0.943203  \n",
       "\n",
       "[160 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the created dataframe and have a look at it to plot it later for the analysis\n",
    "param_testing = pd.read_csv(\"param_testing_nnet.csv\", sep=\";\")\n",
    "param_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function for barplots for some of the parameters to compare mae and r2\n",
    "def plot_bars(data_filter):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(16, 8))\n",
    "    plot_data = param_testing[param_testing[\"Parameter\"] == data_filter]\n",
    "\n",
    "    mae_train_axes = axes[0, 0]\n",
    "    sns.barplot(data=plot_data, x=\"ParameterValue\", y=\"MaeTrain\", ax=mae_train_axes, palette=\"rocket\")\n",
    "    mae_train_axes.set_ylim(200, 600)\n",
    "\n",
    "    mae_test_axes = axes[0, 1]\n",
    "    sns.barplot(data=plot_data, x=\"ParameterValue\", y=\"MaeTest\", ax=mae_test_axes, palette=\"rocket\")\n",
    "    mae_test_axes.set_ylim(200, 600)\n",
    "\n",
    "    r2_train_axes = axes[1, 0]\n",
    "    sns.barplot(data=plot_data, x=\"ParameterValue\", y=\"R2Train\", ax=r2_train_axes, palette=\"Blues\")\n",
    "    r2_train_axes.set_ylim(0.9, 1)\n",
    "\n",
    "    r2_test_axes = axes[1, 1]\n",
    "    sns.barplot(data=plot_data, x=\"ParameterValue\", y=\"R2Test\", ax=r2_test_axes, palette=\"Blues\")\n",
    "    r2_test_axes.set_ylim(0.9, 1)\n",
    "\n",
    "# apply the function to wanted parameters for comparison\n",
    "plot_bars(\"hidden_layer_size\")\n",
    "plot_bars(\"activation\")\n",
    "plot_bars(\"solver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Interpretation:* <br> - Hidden Layers: It can be seen that \"(100,)\" or \"(32, 64, 32)\" are the best parameters while analyzing MAE and R^2 <br> - Activation: Clearly \"Relu\" or \"Logistic\" should be used as an activation function. <br> - Solver: \"Adam\" is the better solver on the training an the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function for lineplots for some of the parameters to compare mae and r2\n",
    "def plot_lines(data_filter):\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(16, 8))\n",
    "    plot_data = param_testing[param_testing[\"Parameter\"] == data_filter]\n",
    "    plot_data = pd.melt(plot_data, id_vars=[\"Parameter\", \"ParameterValue\"], var_name=\"Measure\", \n",
    "    value_vars=[\"MaeTrain\", \"MaeTest\", \"R2Train\", \"R2Test\"], value_name=\"MeasuredValue\")\n",
    "\n",
    "    mae_data = plot_data[plot_data[\"Measure\"].isin([\"MaeTrain\", \"MaeTest\"])]\n",
    "    mae_axes = axes[0]\n",
    "    sns.lineplot(data=mae_data, x=\"ParameterValue\", y=\"MeasuredValue\", hue=\"Measure\", ax=mae_axes, palette=\"rocket\")\n",
    "    mae_axes.set_xticks([0, 24, 49])\n",
    "\n",
    "    r2_data = plot_data[plot_data[\"Measure\"].isin([\"R2Train\", \"R2Test\"])]\n",
    "    r2_axes = axes[1]\n",
    "    sns.lineplot(data=r2_data, x=\"ParameterValue\", y=\"MeasuredValue\", hue=\"Measure\", ax=r2_axes, palette=\"Blues\")\n",
    "    r2_axes.set_xticks([0, 24, 49])\n",
    "\n",
    "# apply the function to wanted parameters for comparison\n",
    "plot_lines(\"alpha\")\n",
    "plot_lines(\"learning_rate_init\")\n",
    "plot_lines(\"max_iter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Interpretation:* <br> - Alpha: It should be somewhere in the middle of 0.00001 and 0.001, so the default value of 0.0001 doesn't seem to be too bad <br> - Learning Rate: A good learning rate seems to be around 0.001, like the default value. Maybe we should event try something smaller <br> - Iterations: There is no different in using 1000 or 2000 iterations. We try it nevertheless to find out if it is influenced by other parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*3. Trying to optimize the Hyperparameters of the neural network using the findings above*\n",
    "1. The optimal tuning of the hyperparameters would normally be done with the whole dataset. This would take for this project far too long to calculate. Therefore the data is subsetted and 10% of the data is used for the hyperparameter tuning. \n",
    "2. First using RandomizedSearchCV on the subset to find a good starting point for each parameter.\n",
    "3. With the results use GridSearchCV and conduct an analysis in a range around the found parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsetting the data for the optimization of the hyperparameters\n",
    "ped_subset = ped.sample(frac=0.1, random_state=1)\n",
    "\n",
    "# Define a new X of the subset with the squared feature k = 0.99, 57+46\n",
    "X_subset = np.array(ped_subset[ped_subset.columns[57:103]])\n",
    "\n",
    "# Subset of the output to predict\n",
    "y_subset = ped_subset[\"pedestrians count\"]\n",
    "\n",
    "# Split the subset into train and test sets\n",
    "Xtrain_subset, Xtest_subset, ytrain_subset, ytest_subset = train_test_split(\n",
    "    X_subset, y_subset, test_size=0.3, random_state=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data of the subset, so it is less sensitive to feature scaling \n",
    "# scale the inputs\n",
    "scaler.fit(Xtrain_subset)\n",
    "Xtrain_scaled_subset = scaler.transform(Xtrain_subset)\n",
    "# apply the same transformation to the test data to have meaningful results\n",
    "scaler.fit(Xtest_subset)\n",
    "Xtest_scaled_subset = scaler.transform(Xtest_subset)\n",
    "# scale the y different because we can't use standardscaler to a 1d array\n",
    "mu_subset, sigma_subset = y_subset.mean(), y_subset.std()\n",
    "ytest_scaled_subset = (ytest_subset - mu_subset) / sigma_subset\n",
    "ytrain_scaled_subset = (ytrain_subset - mu_subset) /sigma_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the model and the parameters used in RandomizedSearchCV\n",
    "nnet_randomsearch = MLPRegressor(random_state=72)\n",
    "\n",
    "parameters_randomsearch = {\n",
    "    \"hidden_layer_sizes\": [(50,), (100,), (150,), (9, 18, 6), (32, 64, 32), (12, 24, 12)],\n",
    "    \"activation\": [\"logistic\", \"relu\"],\n",
    "    \"solver\": [\"adam\"],\n",
    "    \"alpha\": np.linspace(0.00005, 0.0005, 20),\n",
    "    \"learning_rate_init\" : np.linspace(0.0005, 0.001, 20),\n",
    "    \"max_iter\" : range(1000, 2000, 100)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the RandomizedSearchCV\n",
    "nnet_rndm_src = RandomizedSearchCV(estimator=nnet_randomsearch, param_distributions=parameters_randomsearch, cv=2)\n",
    "nnet_rndm_src.fit(Xtrain_subset, ytrain_subset)\n",
    "nnet_rndm_src.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*results:* <br> {'solver': 'adam', <br>\n",
    " 'max_iter': 1400, <br>\n",
    " 'learning_rate_init': 0.0006052631578947369, <br>\n",
    " 'hidden_layer_sizes': (9, 18, 6), <br>\n",
    " 'alpha': 0.00047631578947368423, <br>\n",
    " 'activation': 'relu'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>mae_train</th>\n",
       "      <th>mae_test</th>\n",
       "      <th>r2_train</th>\n",
       "      <th>r2_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nnet_base</td>\n",
       "      <td>347.447001</td>\n",
       "      <td>456.576090</td>\n",
       "      <td>0.964814</td>\n",
       "      <td>0.942335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nnet_randomsearch</td>\n",
       "      <td>349.732476</td>\n",
       "      <td>457.666502</td>\n",
       "      <td>0.964785</td>\n",
       "      <td>0.941682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model   mae_train    mae_test  r2_train   r2_test\n",
       "0          nnet_base  347.447001  456.576090  0.964814  0.942335\n",
       "1  nnet_randomsearch  349.732476  457.666502  0.964785  0.941682"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a neural network with those results and look at mae and r2\n",
    "nnet_random = MLPRegressor(hidden_layer_sizes=(9, 18, 6), activation=\"relu\", solver=\"adam\", alpha=0.00047631578947368423,\n",
    "    learning_rate_init=0.0006052631578947369, max_iter=1400, random_state=72)\n",
    "\n",
    "# Fit the network to the scaled train data\n",
    "nnet_base.fit(Xtrain_scaled, ytrain_scaled)\n",
    "\n",
    "# Make predictions\n",
    "ypred_train_scaled_random = nnet_base.predict(Xtrain_scaled)\n",
    "ypred_scaled_random = nnet_base.predict(Xtest_scaled)\n",
    "\n",
    "# Reconstruct outputs and scale back predictions\n",
    "ypred_train_random = ypred_train_scaled_random * sigma + mu\n",
    "ypred_random = ypred_scaled_random * sigma + mu\n",
    "# the network predicts negative values because it is not possible to put an activation function on the output layer\n",
    "# MLPRegressor uses the activation function only on the hidden layers\n",
    "# Negative Values make no sense in our case, therefore we fix it and tell the network that all negative values\n",
    "# should equal to zero\n",
    "ypred_random[ypred_random < 0] = 0\n",
    "\n",
    "# Compute the MAE\n",
    "mae_train_random = mean_absolute_error(ytrain, ypred_train_random)\n",
    "mae_test_random = mean_absolute_error(ytest, ypred_random)\n",
    "# Comute R^2\n",
    "r2_train_random = r2_score(ytrain, ypred_train_random)\n",
    "r2_test_random = r2_score(ytest, ypred_random)\n",
    "\n",
    "# adding the calculated reults to the dataframe\n",
    "row = pd.DataFrame(data=[[\"nnet_randomsearch\",  mae_train_random, mae_test_random, r2_train_random, r2_test_random]],\n",
    "columns=[\"model\", \"mae_train\", \"mae_test\", \"r2_train\", \"r2_test\"])\n",
    "comparison = pd.concat([comparison, row])\n",
    "# merging results into dataframe and safe it as csv to add things later\n",
    "comparison = comparison.reset_index().drop(columns=\"index\")\n",
    "comparison.to_csv(\"nnet_comparison.csv\", sep=\";\", index=False)\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data so it is less sensitive to feature scaling\n",
    "scaler = StandardScaler()\n",
    "# scale the inputs\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "# scale the y different because we can't use standardscaler to a 1d array\n",
    "mu, sigma = y.mean(), y.std() # We will use this to scale back to original values!\n",
    "y = (y - mu) / sigma\n",
    "\n",
    "scorers = [\"r2\", \"neg_mean_absolute_error\"]\n",
    "\n",
    "parameters= {\n",
    "    \"hidden_layer_sizes\": [(32, 64, 32), (64, 128, 64), (32, 64, 128, 64, 32)],\n",
    "    \"activation\": [\"tanh\", \"relu\"],\n",
    "    \"solver\": [\"sgd\", \"adam\"],\n",
    "    \"alpha\": [0.0001, 0.0005, 0.00005],\n",
    "    \"learning_rate_init\" : [0.001, 0.01],\n",
    "    \"max_iter\" : [1000, 2000]\n",
    "}\n",
    "\n",
    "model1 = GridSearchCV(MLPRegressor(), parameters, cv=5, scoring=scorers, refit=False)\n",
    "model1.fit(X, y)\n",
    "\n",
    "results = pd.DataFrame()\n",
    "params = model1.cv_results_[\"params\"]\n",
    "\n",
    "maes = model1.cv_results_[\"mean_test_neg_mean_absolute_error\"]\n",
    "r2s = model1.cv_results_[\"mean_test_r2\"]\n",
    "\n",
    "results[\"Params\"] = pd.Series(params)\n",
    "results[\"MeanMAE\"] = pd.Series(maes)\n",
    "results[\"MeanR2\"] = pd.Series(r2s)\n",
    "\n",
    "results.sort_values(\"MeanR2\", inplace=True)\n",
    "\n",
    "results.to_csv(\"results_nnet_gs.csv\", sep=\";\", index=False)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelling the MLPRegressor with best parameters\n",
    "# importing the data and splitting it to work with\n",
    "# important!! use index_col=0 to read df otherwise pca starts at col 58\n",
    "ped = pd.read_csv(\"data_clean_with_dummies.csv\", index_col=0)\n",
    "\n",
    "# beginning of the code, so everyone has the same\n",
    "np.random.seed(1) # Set the random seed for reproduceability\n",
    "\n",
    "# Define a new X with the squared feature k = 0.99, 57+46\n",
    "X = np.array(ped[ped.columns[57:103]])\n",
    "\n",
    "# Output to predict\n",
    "y = ped[\"pedestrians count\"]\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a neural network with the standard parameters \n",
    "nnet_optimization = MLPRegressor(hidden_layer_sizes=(32, 64, 32), activation=\"relu\", solver=\"adam\",\n",
    "                    alpha=0.0001, learning_rate_init=0.01, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data to work so it is less sensitive to feature scaling \n",
    "scaler = StandardScaler()\n",
    "# scale the inputs\n",
    "scaler.fit(Xtrain)\n",
    "Xtrain = scaler.transform(Xtrain)\n",
    "# apply the same transformation to the test data to have meaningful results\n",
    "Xtest = scaler.transform(Xtest)\n",
    "# scale the y different because we can't use standardscaler to a 1d array\n",
    "mu, sigma = y.mean(), y.std() # We will use this to scale back to original values!\n",
    "ytest = (ytest - mu) / sigma\n",
    "ytrain = (ytrain - mu) /sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the network to the train data\n",
    "nnet_optimization.fit(Xtrain, ytrain)\n",
    "\n",
    "# Make predictions\n",
    "ypred_train = nnet_optimization.predict(Xtrain)\n",
    "ypred = nnet_optimization.predict(Xtest)\n",
    "\n",
    "# Reconstruct outputs and scale predictions\n",
    "ytest = ytest * sigma + mu\n",
    "ytrain = ytrain * sigma + mu\n",
    "ypred_train = ypred_train * sigma + mu\n",
    "ypred = ypred * sigma + mu\n",
    "\n",
    "# Compute the MAE\n",
    "mae_train = mean_absolute_error(ytrain, ypred_train)\n",
    "mae_test = mean_absolute_error(ytest, ypred)\n",
    "# Comute R^2\n",
    "r2_train = r2_score(ytrain, ypred_train)\n",
    "r2_test = r2_score(ytest, ypred)\n",
    "\n",
    "print(f\"The mean absolute error of the training data is {mae_train:>10.2f}\")\n",
    "print(f\"The R^2 of the training data is {r2_train:>10.2f}\")\n",
    "print(20 * \"*\")\n",
    "print(f\"The mean absolute error of the testing data is {mae_test:>10.2f}\")\n",
    "print(f\"The R^2 of the testing data is {r2_test:>10.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Results:* <br> The mean absolute error of the training data is 339.24 <br>\n",
    "The R^2 of the training data is 0.96 <br>\n",
    "******************** <br>\n",
    "The mean absolute error of the testing data is 400.66 <br>\n",
    "The R^2 of the testing data is 0.94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data so it is less sensitive to feature scaling\n",
    "scaler = StandardScaler()\n",
    "# scale the inputs\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "# scale the y different because we can't use standardscaler to a 1d array\n",
    "mu, sigma = y.mean(), y.std() # We will use this to scale back to original values!\n",
    "y = (y - mu) / sigma\n",
    "\n",
    "scorers = [\"r2\", \"neg_mean_absolute_error\"]\n",
    "\n",
    "parameters= {\n",
    "    \"hidden_layer_sizes\": [(32, 64, 32), (100,)],\n",
    "    \"activation\": [\"relu\"],\n",
    "    \"solver\": [\"adam\"],\n",
    "    \"alpha\": [0.0001],\n",
    "    \"learning_rate_init\" : [0.001],\n",
    "    \"batch_size\" : [16, 32, 64],\n",
    "    \"max_iter\" : [1000]\n",
    "}\n",
    "\n",
    "model1 = GridSearchCV(MLPRegressor(), parameters, cv=5, scoring=scorers, refit=False)\n",
    "model1.fit(X, y)\n",
    "\n",
    "results = pd.DataFrame()\n",
    "params = model1.cv_results_[\"params\"]\n",
    "\n",
    "maes = model1.cv_results_[\"mean_test_neg_mean_absolute_error\"]\n",
    "r2s = model1.cv_results_[\"mean_test_r2\"]\n",
    "\n",
    "results[\"Params\"] = pd.Series(params)\n",
    "results[\"MeanMAE\"] = pd.Series(maes)\n",
    "results[\"MeanR2\"] = pd.Series(r2s)\n",
    "\n",
    "results.sort_values(\"MeanR2\", inplace=True)\n",
    "\n",
    "results.to_csv(\"results_nnet_gs2.csv\", sep=\";\", index=False)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('dsf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f4c25ce9dd1a50de927298dc55a371fca9b9bb0e26aa91c5573ba8ea5976be0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
